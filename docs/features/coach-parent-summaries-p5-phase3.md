# Phase 5.3 (Cost Optimization)

> Auto-generated documentation - Last updated: 2026-01-24 23:51

## Status

- **Branch**: `ralph/coach-parent-summaries-p5-phase3`
- **Progress**: 4 / 4 stories complete
- **Phase Status**: âœ… Complete

## Completed Features

### US-012: Add prompt caching to generateParentSummary action

As a platform, I reduce AI costs by 90% through Anthropic prompt caching.

**Acceptance Criteria:**
- Edit packages/backend/convex/actions/generateParentSummary.ts
- Add anthropic-beta header: 'anthropic-beta': 'prompt-caching-2024-07-31'
- Structure messages array to cache system prompt and player/sport context
- Only insight content varies (no cache on that)
- Extract cache statistics from API response: usage.cache_creation_input_tokens, usage.cache_read_input_tokens
- Update cost calculation: (input_tokens - cached_tokens) * 0.000005 + cached_tokens * 0.0000005 + output_tokens * 0.000015
- Return cache stats in action response for logging
- Typecheck passes: npm run check-types

### US-013: Add aiUsageLog table to schema

As a platform, I track every AI call for cost visibility and analytics.

**Acceptance Criteria:**
- Edit packages/backend/convex/schema.ts
- Add new table aiUsageLog with fields:
-   timestamp: v.number() - Date.now() when call made
-   organizationId: v.id('organization') - Which org incurred the cost
-   coachId: v.string() - Which coach triggered the call
-   playerId: v.optional(v.id('orgPlayerEnrollments')) - Which player (if applicable)
-   operation: v.string() - Type of operation: 'parent_summary', 'voice_note_transcription', etc.
-   model: v.string() - Model used: 'claude-3-haiku-20240307'
-   inputTokens: v.number() - Total input tokens
-   cachedTokens: v.number() - Cached input tokens (0 if no cache)
-   outputTokens: v.number() - Output tokens generated
-   cost: v.number() - Cost in dollars (e.g., 0.00015)
-   cacheHitRate: v.number() - Percentage cached (cachedTokens / inputTokens)
- Add indexes: by_organizationId, by_coachId, by_timestamp, by_operation
- Run: npx -w packages/backend convex codegen
- Typecheck passes: npm run check-types

### US-014: Log AI usage in generateParentSummary action

As a platform, every AI call is logged with token counts and costs.

**Acceptance Criteria:**
- Create packages/backend/convex/models/aiUsageLog.ts
- Add mutation logUsage with args: timestamp, organizationId, coachId, playerId, operation, model, inputTokens, cachedTokens, outputTokens, cost, cacheHitRate
- Edit packages/backend/convex/actions/generateParentSummary.ts
- After successful API call, extract token counts from response.usage
- Calculate cost: (inputTokens - cachedTokens) * 0.000005 + cachedTokens * 0.0000005 + outputTokens * 0.000015
- Calculate cache hit rate: cachedTokens / inputTokens (0-1 range)
- Call ctx.runMutation(internal.models.aiUsageLog.logUsage, { ... }) with all fields
- Include error handling: if logging fails, log to console but don't fail the summary creation
- Typecheck passes: npm run check-types

### US-015: Create AI usage analytics dashboard query

As an org admin, I see AI usage and costs for my organization.

**Acceptance Criteria:**
- In packages/backend/convex/models/aiUsageLog.ts, add query getOrgUsage
- Args: organizationId, startDate (optional), endDate (optional)
- Use by_organizationId index with filter for date range
- Calculate aggregates:
-   totalCost: sum of all cost fields
-   totalInputTokens: sum of inputTokens
-   totalCachedTokens: sum of cachedTokens
-   totalOutputTokens: sum of outputTokens
-   averageCacheHitRate: average of cacheHitRate
-   callCount: number of log entries
- Group by operation type with breakdown
- Top 5 coaches by usage
- Top 5 players by usage
- Add returns validator with full analytics structure
- Typecheck passes: npm run check-types


## Implementation Notes

### Key Patterns & Learnings


## Key Files


---
*Documentation auto-generated by Ralph Documenter Agent*
